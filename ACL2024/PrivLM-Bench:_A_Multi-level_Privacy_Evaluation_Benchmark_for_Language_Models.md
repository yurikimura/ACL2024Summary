この論文は、PrivLM-Benchという新しい評価ベンチマークを提案しています。これは、大規模言語モデル（LMs）が実際の使用中にどれだけプライベートなデータを漏らす可能性があるかを、実証的かつ直観的に測定するためのものです。

主なポイントは次の通りです。

1. 従来の評価方法の限界：従来は主に差分プライバシー（DP）のパラメータ（例：ε, δ）を報告してモデルのプライバシーを評価していた。しかしこれらのパラメータだけでは、実際の運用中にデータ漏洩がどれだけ起きるかを十分に反映できていません。

2. 実態に即した評価：これに対し、PrivLM-Benchは実際のモデル使用時に行われるさまざまな「プライバシー攻撃（attack）」を実行します。これにより、モデルの情報漏洩リスクをより現実的に測定します。

3. 多角的なプライバシー目的の設定：モデルのfine-tuningにおいて、データ抽出攻撃、メンバーシップ推定攻撃、埋め込みレベル攻撃など複数の攻撃手法を用意し、評価を包括的に行います。

4. 比較と評価：これらの攻撃結果をもとに、さまざまなプライバシー保護手法やモデルのプライバシー安全性を比較・検証します。

5. 意義と目的：このベンチマークは、既存のプライバシー保護策（例：差分プライバシー）が本当にプライバシーを守っているかどうかを、実態の攻撃シナリオで検証できるようになっています。

要するに、PrivLM-Benchは、モデルの「理論的なプライバシー指標」だけではなく、「実際に攻撃してみたときの漏洩リスク」を重視した、新しいプライバシー評価の枠組みを提供しているのです。

具体的に使われる攻撃手法には、データ抽出攻撃やメンバーシップ推定攻撃、埋め込み逆推定攻撃などがあります。これらの手法を用いて、モデルのプライバシーリスクを多角的に評価し、より安全なモデルの設計や運用に役立てることを目的としています。

PrivLM-Benchでは、主に以下の3つのタイプのプライバシーアタックを行っています。

1. データ抽出攻撃（Data Extraction Attacks）

- 目的：訓練データや敏感な情報の一部をモデルから抽出すること。

- 方法：モデルに対して特定のプレフィックス（部分的なパターン）を入力し、その後の出力（サフィックス）を解析して、元のデータに近い内容を復元します。

- 例：訓練に使われた特定の個人情報や秘密のフレーズを引き出そうとします。

1. 参加者推論攻撃（Membership Inference Attacks, MIAs）

- 目的：あるデータポイントが訓練データに含まれていたかどうかを判定すること。

- 方法：モデルの出力または中間表現の信頼度や確信度を利用し、そのデータ点が訓練に使われたか否かを推測します。

- 例：特定の文章が訓練データに含まれていたかを判定します。

1. 埋め込み逆推定攻撃（Embedding Inversion Attacks, EIAs）

- 目的：モデルの埋め込みベクトル（中間層出力）から元のデータを復元すること。

- 方法：高性能なデコーダーを用いて、埋め込みから元の入力テキストや画像などを生成します。

- 例：文章の埋め込みから、その文章のテキストを再構築します。

これらの攻撃は、モデルが学習データや推論時の入力情報を漏洩させるリスクを評価するために行われます。PrivLM-Benchは、こうした多様な攻撃方法を統一的なパイプラインで実施し、モデルのプライバシー安全性を多角的に検証しています。
